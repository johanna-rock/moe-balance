# Simulation framework for balancing MOE experts

Target model: Deepseek v3

Hardware
- 128 devices in 16x8 grid

Target scheme
- 384 experts on 128 devices -> 3 experts per device
  - 256 normal experts, 1 shared expert (16 relications), 112 hot expert replications

Goal
- Find the most efficient expert balancing strategy to optimize for low latency

Optimization levels
- Optimized MOE op implementation
- Optimize balancing by optimizing expert placement (mix hot+cold experts)
- Optimize balancing by optimizing hot expert replication and placement (treat shared expert as hot expert, repliacte hot experts)
- Optimize balancing by capacity factor and rerouting: if capacity per expert is reached, re-route tokens to next top expert instead

Model (DeepSeek V3 / R1)
- Architecture: MoE transformer with 671B total parameters and ~37B active per token.
- MoE routing: 256 routed experts plus 1 shared expert per MoE layer; each token activates the shared expert + top-8 routed experts.
- Load balancing: auxiliary-loss-free routing with per-expert bias terms (important for understanding non-uniform expert utilization).
- R1 lineage: DeepSeek-R1 and R1-Zero are trained on the DeepSeek-V3-Base architecture (same MoE layout and activation budget).

Implementation details
See `docs/functional_moe_implementation.md` for the functional implementation and `docs/optimized_moe_implementation.md` for the optimized implementation.

Data
For each request in the dataset, there is an npz file that contains the following: request_id, prompt_token_ids, output_token_ids, routed_experts
request_id: request ID found in the dataset or row number in case no ID is found
prompt_token_ids:  actual input tokens for that prompt
output_token_ids: actual output tokens generated by the model
routed_experts: a 3-dimensional array defined as [sequence length, layer number, expert ids], where sequence length = input tokens + output tokens

Approach to find best balancing strategy
1. Import MOE expert selection statistics (average frequence of selected experts per dataset)
2. Basline: compute minimum avg tokens over all devices
    Per dataset, per layer:
        1.Find best placement by pairing hottest/coldest until all pairs found
3. Compute best placement/replication with 384 slots based on frequencies
    Per dataset, per layer:
        1. Loop over all free solts (128) and pick the expert with the highest frequency -> replicate
        2. Find best placement by pairing hottest/coldest until all pairs found
3. Get data per batch: for each user/fwd pass, get the active experts
4. Frame as scheduling problem
    - Run simulation of active experts per batch (random shuffling for batch)
        - pass in a mapping between expert id and expert location (fixed slots on device)
            - contains replication (if two mappings from expert if to slots -> assign round robin)
            - contains placement
        - simulate the inputs/aktive experts with the given mapping
        - brute force over all combinations or do some smart scheduling algorithm
5. Run experiments
    Per dataset, per layer:
        1. 256 experts, find placement
        2. 384 experts incl. shared expert in data, find placement/replication

General scheduling rules/ideas:
- Replicated experts on different rows to minimize inter-row traffic
- Place co-selected experts in the same rows

More promising strategies

  1. Trace‑driven objective + constrained optimization
      - Use real routing traces (token → top‑k experts) and simulate A2A traffic, local compute, queueing.
      - Objective: minimize p95/p99 latency or max per‑device load (compute + communication).
      - Constraints: per‑device expert slots, per‑row bandwidth, replication budget.
      - Solve with heuristics: greedy + local search + simulated annealing.
  2. Two‑stage optimization (replication then placement)
      - Stage A: Choose replicas to minimize expected max load: allocate replicas proportional to heavy‑tail + burstiness (e.g., mean +
        α·std or p95).
      - Stage B: Place experts/replicas with a graph partitioning objective:
          - Build a co‑activation graph (edge weight = frequency of expert i and j co‑selected).
          - Partition onto devices/rows to keep heavy co‑activation local.
  3. ILP / CP‑SAT for smaller subproblems
      - Formulate smaller chunks (e.g., per‑row placement or per‑layer) as integer programs.
      - This yields a high‑quality baseline and informs heuristics for full‑scale runs.
  4. Load‑aware replica routing
      - Instead of round‑robin, route tokens to the replica on the least‑loaded row/device.
      - With a lightweight “load hint” per device, this can drastically cut tail latency.
  5. Row‑aware placement
      - Since A2A dispatch is along DP rows, reduce inter‑row traffic by placing frequently co‑selected experts in the same row.
      - For the shared expert, replicate across rows (not just devices) to reduce row‑to‑row exchange.
      - Simulate with a capacity factor per expert; treat capacity as a tunable parameter alongside placement/replication.
  7. Metaheuristics with fast surrogate cost model
      - Build a quick cost estimator (compute+comm) and use:
          - hill‑climb / tabu search
      - Use full simulation only for top candidates.

  A more optimized overall strategy (suggested)
  2. Metrics: Compute per‑expert mean, variance, p95 load; build co‑activation matrix.
  5. Replica routing policy:
      - Prefer least‑loaded replica on the same row; if saturated, spill to other rows.
  6. Simulation loop:
      - Run trace‑driven sim; evaluate p95/p99 latency and max device load.
  7. Local search:
      - Swap experts/replicas between devices/rows to reduce the worst‑case load.
  8. Finalize:
      - Validate against multiple datasets; pick robust placement (not just best on one trace).
